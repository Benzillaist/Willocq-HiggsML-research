{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 14:15:19.506012: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-18 14:15:21.043210: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/benk/root/root-6.26.06-install/lib\n",
      "2022-10-18 14:15:21.043305: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-18 14:15:21.295215: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-18 14:15:27.446209: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/benk/root/root-6.26.06-install/lib\n",
      "2022-10-18 14:15:27.446508: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/benk/root/root-6.26.06-install/lib\n",
      "2022-10-18 14:15:27.446528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import uproot\n",
    "import pandas as pd\n",
    "import awkward as ak\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import tensorflow_decision_forests as tfdf\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "from keras.callbacks import LearningRateScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def open_root_files(file_names,tree):\n",
    "    file = uproot.open(file_names)\n",
    "    tree_name = file[tree]\n",
    "    return tree_name\n",
    "\n",
    "def create_heat_map(df):\n",
    "    corr = df.corr()\n",
    "    sns.heatmap(corr, \n",
    "    cmap='RdYlGn', \n",
    "    xticklabels=corr.columns.values,\n",
    "    yticklabels=corr.columns.values)\n",
    "    plt.show()\n",
    "\n",
    "def create_tensor_object(train_variables,dict):\n",
    "    df = pd.DataFrame()\n",
    "    key = list(dict.keys())[0]\n",
    "    for var in train_variables:\n",
    "        if var == \"classification\":\n",
    "            continue\n",
    "        else:\n",
    "            #df[var] = np.array(dict[key][var].array())\n",
    "            df.insert(len(df.columns), var, dict[key][var].array())\n",
    "            print(dict[key][var].array())\n",
    "    if \"signal\" in key:\n",
    "        print(\"SIGNAL CLASSIFICATION SET TO 1\", key)\n",
    "        df.insert(0, 'classification', 1)\n",
    "    else:\n",
    "        print(\"BACKGROUND CLASSIFICATION SET TO 0\", key)\n",
    "        df.insert(0, 'classification', 0)\n",
    "    \n",
    "    #split the data into train and testing set\n",
    "    labels = df.iloc[:,0]\n",
    "    features = df.iloc[:, 1:]\n",
    "    \n",
    "    train_df_1, test_df, labels_train_1, labels_test = train_test_split(features, labels, test_size=0.2)\n",
    "    \n",
    "    train_df, val_df, labels_train, labels_val = train_test_split(train_df_1, labels_train_1, test_size=0.1)\n",
    "   \n",
    "    # numerical_features = features.select_dtypes(include=['float64', 'int64', 'float32', 'int32'])\n",
    "    # numerical_columns = numerical_features.columns\n",
    "    # print(type(numerical_columns))\n",
    "    ct = ColumnTransformer([(\"only numeric\", StandardScaler(), [0,12])], remainder = 'passthrough')\n",
    "    features_train_scaled = ct.fit_transform(train_df)\n",
    "    features_val_scaled = ct.transform(val_df)\n",
    "    features_test_scaled = ct.transform(test_df) \n",
    "    print(features_train_scaled.shape)   \n",
    "    \n",
    "    #create heat map of training variables\n",
    "    hmap = create_heat_map(train_df)\n",
    "    \n",
    "    # return features_train_scaled, features_test_scaled, labels_train, labels_test, features_val_scaled, labels_val\n",
    "    return train_df, test_df, labels_train, labels_test, val_df, labels_val\n",
    "    #return 0, 0, 0, 0, 0, 0\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loss(fit):\n",
    "    plt.plot(fit.history['loss'])\n",
    "    plt.plot(fit.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(fit):\n",
    "    plt.plot(fit.history['accuracy'])\n",
    "    plt.plot(fit.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def custom_LearningRate_schedular(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    else:\n",
    "        return 0.01 * tf.math.exp(0.1 * (10 - epoch))\n",
    "\n",
    "    \n",
    "def get_model(train_tensor):\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu', input_shape=(len(train_tensor[0])-1,)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    # keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    # keras.layers.Dense(4, activation='relu'),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.optimizers.SGD(learning_rate=0.0001),\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(\n",
    "                        name='binary_crossentropy'),\n",
    "                metrics=['accuracy', \n",
    "                        keras.metrics.AUC(name='auc'),\n",
    "                        keras.metrics.AUC(name='prc', curve='PR')])\n",
    "    return model\n",
    "\n",
    "def boosted_decision_tree():\n",
    "    model = tfdf.keras.GradientBoostedTreesModel()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the files\n",
    "signal_file_e = open_root_files(\"mc16e_signal.root\",\"nominal\")\n",
    "#signal_file_d = open_root_files(\"~/Physics/HWWAnalysis/ntuples/mc16d_signal.root\",\"nominal\")\n",
    "#signal_file_a = open_root_files(\"~/Physics/HWWAnalysis/ntuples/mc16a_signal.root\",\"nominal\")\n",
    "ttbar_file_e = open_root_files(\"mc16e_ttbar.root\",\"nominal\")\n",
    "#ttbar_file_d = open_root_files(\"~/Physics/HWWAnalysis/ntuples/mc16d_ttbar.root\",\"nominal\")\n",
    "#ttbar_file_a = open_root_files(\"~/Physics/HWWAnalysis/ntuples/mc16a_ttbar.root\",\"nominal\")\n",
    "#wjets_file_e = open_root_files(\"~/Physics/HWWAnalysis/ntuples/mc16e_wjets.root\",\"nominal\")\n",
    "#wjets_file_d = open_root_files(\"~/Physics/HWWAnalysis/ntuples/mc16d_wjets.root\",\"nominal\")\n",
    "#wjets_file_a = open_root_files(\"~/Physics/HWWAnalysis/ntuples/mc16a_wjets.root\",\"nominal\")\n",
    "#diboson_file_e = open_root_files(\"~/Physics/HWWAnalysis/ntuples/mc16e_diboson.root\",\"nominal\")\n",
    "#diboson_file_d = open_root_files(\"~/Physics/HWWAnalysis/ntuples/mc16d_diboson.root\",\"nominal\")\n",
    "#diboson_file_a = open_root_files(\"~/Physics/HWWAnalysis/ntuples/mc16a_diboson.root\",\"nominal\")\n",
    "#zjets_file_e = open_root_files(\"~/Physics/HWWAnalysis/ntuples/mc16e_zjets.root\",\"nominal\")\n",
    "#zjets_file_d = open_root_files(\"~/Physics/HWWAnalysis/ntuples/mc16d_zjets.root\",\"nominal\")\n",
    "#zjets_file_a = open_root_files(\"~/Physics/HWWAnalysis/ntuples/mc16a_zjets.root\",\"nominal\")\n",
    "#singletop_file_e = open_root_files(\"~/Physics/HWWAnalysis/ntuples/mc16e_singletop.root\",\"nominal\")\n",
    "#singletop_file_d = open_root_files(\"~/Physics/HWWAnalysis/ntuples/mc16d_singletop.root\",\"nominal\")\n",
    "#singletop_file_a = open_root_files(\"~/Physics/HWWAnalysis/ntuples/mc16a_singletop.root\",\"nominal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set a frame with no defined index and a value that cannot be converted to a Series",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4233\u001b[0m, in \u001b[0;36mDataFrame._ensure_valid_index\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 4233\u001b[0m     value \u001b[39m=\u001b[39m Series(value)\n\u001b[1;32m   4234\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:387\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    385\u001b[0m name \u001b[39m=\u001b[39m ibase\u001b[39m.\u001b[39mmaybe_extract_name(name, data, \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m))\n\u001b[0;32m--> 387\u001b[0m \u001b[39mif\u001b[39;00m is_empty_data(data) \u001b[39mand\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# gh-17261\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    390\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe default dtype for empty Series will be \u001b[39m\u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mof \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfloat64\u001b[39m\u001b[39m'\u001b[39m\u001b[39m in a future version. Specify a dtype explicitly \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[1;32m    395\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/construction.py:878\u001b[0m, in \u001b[0;36mis_empty_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    877\u001b[0m is_list_like_without_dtype \u001b[39m=\u001b[39m is_list_like(data) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 878\u001b[0m is_simple_empty \u001b[39m=\u001b[39m is_list_like_without_dtype \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;49;00m data\n\u001b[1;32m    879\u001b[0m \u001b[39mreturn\u001b[39;00m is_none \u001b[39mor\u001b[39;00m is_simple_empty\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/awkward/highlevel.py:1489\u001b[0m, in \u001b[0;36mArray.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1489\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1490\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe truth value of an array whose length is not 1 is ambiguous; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1491\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39muse ak.any() or ak.all()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1492\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: the truth value of an array whose length is not 1 is ambiguous; use ak.any() or ak.all()",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m train_variables \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmupt_cand\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmueta_cand\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmuphi_cand\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mljet_pt_cand\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mljet_eta_cand\u001b[39m\u001b[39m'\u001b[39m,\\\n\u001b[1;32m      2\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mljet_phi_cand\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mljet_mass_cand\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdR_values_cand\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpt_higgs\u001b[39m\u001b[39m'\u001b[39m,\\\n\u001b[1;32m      3\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mmass_T\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmet_met\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmet_phi\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmass_mj\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[39m#make the training and testing samples for each period\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m signal_train_e,  signal_test_e, signal_train_out_e, signal_test_out_e, signal_val_e, signal_val_out_e \u001b[39m=\u001b[39m create_tensor_object(train_variables, {\u001b[39m\"\u001b[39m\u001b[39msignal_e\u001b[39m\u001b[39m\"\u001b[39m : signal_file_e})\n",
      "Cell \u001b[0;32mIn [2], line 22\u001b[0m, in \u001b[0;36mcreate_tensor_object\u001b[0;34m(train_variables, dict)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m         \u001b[39m#df[var] = np.array(dict[key][var].array())\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m         df\u001b[39m.\u001b[39;49minsert(\u001b[39mlen\u001b[39;49m(df\u001b[39m.\u001b[39;49mcolumns), var, \u001b[39mdict\u001b[39;49m[key][var]\u001b[39m.\u001b[39;49marray())\n\u001b[1;32m     23\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mdict\u001b[39m[key][var]\u001b[39m.\u001b[39marray())\n\u001b[1;32m     24\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msignal\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m key:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4811\u001b[0m, in \u001b[0;36mDataFrame.insert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   4808\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(loc, \u001b[39mint\u001b[39m):\n\u001b[1;32m   4809\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mloc must be int\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 4811\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[1;32m   4812\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39minsert(loc, column, value)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4895\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4882\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sanitize_column\u001b[39m(\u001b[39mself\u001b[39m, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ArrayLike:\n\u001b[1;32m   4883\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4884\u001b[0m \u001b[39m    Ensures new columns (which go into the BlockManager as new blocks) are\u001b[39;00m\n\u001b[1;32m   4885\u001b[0m \u001b[39m    always copied and converted into an array.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4893\u001b[0m \u001b[39m    numpy.ndarray or ExtensionArray\u001b[39;00m\n\u001b[1;32m   4894\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4895\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_valid_index(value)\n\u001b[1;32m   4897\u001b[0m     \u001b[39m# We can get there through isetitem with a DataFrame\u001b[39;00m\n\u001b[1;32m   4898\u001b[0m     \u001b[39m# or through loc single_block_path\u001b[39;00m\n\u001b[1;32m   4899\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, DataFrame):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4235\u001b[0m, in \u001b[0;36mDataFrame._ensure_valid_index\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4233\u001b[0m         value \u001b[39m=\u001b[39m Series(value)\n\u001b[1;32m   4234\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 4235\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   4236\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot set a frame with no defined index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4237\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mand a value that cannot be converted to a Series\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4238\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   4240\u001b[0m \u001b[39m# GH31368 preserve name of index\u001b[39;00m\n\u001b[1;32m   4241\u001b[0m index_copy \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot set a frame with no defined index and a value that cannot be converted to a Series"
     ]
    }
   ],
   "source": [
    "train_variables = ['classification', 'mupt_cand', 'mueta_cand', 'muphi_cand', 'ljet_pt_cand', 'ljet_eta_cand',\\\n",
    "                   'ljet_phi_cand', 'ljet_mass_cand', 'dR_values_cand', 'pt_higgs',\\\n",
    "                   'mass_T', 'met_met', 'met_phi', 'mass_mj', 'weight']\n",
    "\n",
    "#make the training and testing samples for each period\n",
    "signal_train_e,  signal_test_e, signal_train_out_e, signal_test_out_e, signal_val_e, signal_val_out_e = create_tensor_object(train_variables, {\"signal_e\" : signal_file_e})\n",
    "# signal_train_d,  signal_test_d, signal_train_out_d, signal_test_out_d, signal_val_d, signal_val_out_d = create_tensor_object(train_variables , {\"signal_d\" : signal_file_d})\n",
    "# signal_train_a, signal_test_a, signal_train_out_a, signal_test_out_a, signal_val_a, signal_val_out_a = create_tensor_object(train_variables , {\"signal_a\" : signal_file_a})\n",
    "ttbar_train_e,  ttbar_test_e, ttbar_train_out_e, ttbar_test_out_e, ttbar_val_e, ttbar_val_out_e = create_tensor_object(train_variables , {\"ttbar_e\" : ttbar_file_e})\n",
    "# ttbar_train_d,  ttbar_test_d, ttbar_train_out_d, ttbar_test_out_d, ttbar_val_d, ttbar_val_out_d = create_tensor_object(train_variables , {\"ttbar_d\" : ttbar_file_d})\n",
    "# ttbar_train_a,  ttbar_test_a, ttbar_train_out_a, ttbar_test_out_a, ttbar_val_a, ttbar_val_out_a = create_tensor_object(train_variables , {\"ttbar_a\" : ttbar_file_a})\n",
    "# wjets_train_e,  wjets_test_e, wjets_train_out_e, wjets_test_out_e, wjets_val_e, wjets_val_out_e = create_tensor_object(train_variables , {\"wjets_e\" : wjets_file_e})\n",
    "# wjets_train_d,  wjets_test_d, wjets_train_out_d, wjets_test_out_d, wjets_val_d, wjets_val_out_d = create_tensor_object(train_variables , {\"wjets_d\" : wjets_file_d})\n",
    "# wjets_train_a,  wjets_test_a, wjets_train_out_a, wjets_test_out_a, wjets_val_a, wjets_val_out_a = create_tensor_object(train_variables , {\"wjets_a\" : wjets_file_a})\n",
    "# diboson_train_e,  diboson_test_e, diboson_train_out_e, diboson_test_out_e, diboson_val_e, diboson_val_out_e = create_tensor_object(train_variables , {\"diboson_e\" : diboson_file_e})\n",
    "# diboson_train_d,  diboson_test_d, diboson_train_out_d, diboson_test_out_d, diboson_val_d, diboson_val_out_d = create_tensor_object(train_variables , {\"diboson_d\" : diboson_file_d})\n",
    "# diboson_train_a,  diboson_test_a, diboson_train_out_a, diboson_test_out_a, diboson_val_a, diboson_val_out_a = create_tensor_object(train_variables , {\"diboson_a\" : diboson_file_a})\n",
    "# zjets_train_e,  zjets_test_e, zjets_train_out_e, zjets_test_out_e, zjets_val_e, zjets_val_out_e = create_tensor_object(train_variables , {\"zjets_e\" : zjets_file_e})\n",
    "# zjets_train_d,  zjets_test_d, zjets_train_out_d, zjets_test_out_d, zjets_val_d, zjets_val_out_d = create_tensor_object(train_variables , {\"zjets_d\" : zjets_file_d})\n",
    "# zjets_train_a,  zjets_test_a, zjets_train_out_a, zjets_test_out_a, zjets_val_a, zjets_val_out_a = create_tensor_object(train_variables , {\"zjets_a\" : zjets_file_a})\n",
    "# singletop_train_e, singletop_test_e, singletop_train_out_e, singletop_test_out_e, singletop_val_e, singletop_val_out_e = create_tensor_object(train_variables , {\"singletop_e\" : singletop_file_e})\n",
    "# singletop_train_d,singletop_test_d, singletop_train_out_d, singletop_test_out_d, singletop_val_d, singletop_val_out_d = create_tensor_object(train_variables , {\"singletop_d\" : singletop_file_d})\n",
    "# singletop_train_a, singletop_test_a, singletop_train_out_a, singletop_test_out_a, singletop_val_a, singletop_val_out_a = create_tensor_object(train_variables , {\"singletop_a\" : singletop_file_a})\n",
    "\n",
    "\n",
    "# train_dataset = tf.concat([signal_train_e, signal_train_d, signal_train_a, ttbar_train_e,\\\n",
    "#                             ttbar_train_d, ttbar_train_a, wjets_train_e, wjets_train_d,\\\n",
    "#                             wjets_train_a, diboson_train_e, diboson_train_d, diboson_train_a,\\\n",
    "#                             zjets_train_e, zjets_train_d, zjets_train_a, singletop_train_e,\\\n",
    "#                             singletop_train_d, singletop_train_a], axis=0)\n",
    "\n",
    "\n",
    "# test_dataset = tf.concat([signal_test_e, signal_test_d, signal_test_a, ttbar_test_e,\\\n",
    "#                             ttbar_test_d, ttbar_test_a, wjets_test_e, wjets_test_d,\\\n",
    "#                             wjets_test_a, diboson_test_e, diboson_test_d, diboson_test_a,\\\n",
    "#                             zjets_test_e, zjets_test_d, zjets_test_a, singletop_test_e,\\\n",
    "#                             singletop_test_d, singletop_test_a], axis=0)\n",
    "\n",
    "# val_dataset = tf.concat([signal_val_e, signal_val_d, signal_val_a, ttbar_val_e,\\\n",
    "#                             ttbar_val_d, ttbar_val_a, wjets_val_e, wjets_val_d,\\\n",
    "#                             wjets_val_a, diboson_val_e, diboson_val_d, diboson_val_a,\\\n",
    "#                             zjets_val_e, zjets_val_d, zjets_val_a, singletop_val_e,\\\n",
    "#                             singletop_val_d, singletop_val_a], axis=0)\n",
    "\n",
    "# train_output = tf.concat([signal_train_out_e, signal_train_out_d, signal_train_out_a, ttbar_train_out_e,\\\n",
    "#                             ttbar_train_out_d, ttbar_train_out_a, wjets_train_out_e, wjets_train_out_d,\\\n",
    "#                             wjets_train_out_a, diboson_train_out_e, diboson_train_out_d, diboson_train_out_a,\\\n",
    "#                             zjets_train_out_e, zjets_train_out_d, zjets_train_out_a, singletop_train_out_e,\\\n",
    "#                             singletop_train_out_d, singletop_train_out_a], axis=0)\n",
    "\n",
    "# test_output = tf.concat([signal_test_out_e, signal_test_out_d, signal_test_out_a, ttbar_test_out_e,\\\n",
    "#                             ttbar_test_out_d, ttbar_test_out_a, wjets_test_out_e, wjets_test_out_d,\\\n",
    "#                             wjets_test_out_a, diboson_test_out_e, diboson_test_out_d, diboson_test_out_a,\\\n",
    "#                             zjets_test_out_e, zjets_test_out_d, zjets_test_out_a, singletop_test_out_e,\\\n",
    "#                             singletop_test_out_d, singletop_test_out_a], axis=0)\n",
    "\n",
    "# val_output = tf.concat([signal_val_out_e, signal_val_out_d, signal_val_out_a, ttbar_val_out_e,\\\n",
    "#                             ttbar_val_out_d, ttbar_val_out_a, wjets_val_out_e, wjets_val_out_d,\\\n",
    "#                             wjets_val_out_a, diboson_val_out_e, diboson_val_out_d, diboson_val_out_a,\\\n",
    "#                             zjets_val_out_e, zjets_val_out_d, zjets_val_out_a, singletop_val_out_e,\\\n",
    "#                             singletop_val_out_d, singletop_val_out_a], axis=0)\n",
    "\n",
    "train_dataset = tf.concat([signal_train_e, ttbar_train_e], axis=0)\n",
    "test_dataset = tf.concat([signal_test_e, ttbar_test_e], axis=0)\n",
    "val_dataset = tf.concat([signal_val_e, ttbar_val_e], axis=0)\n",
    "train_output = tf.concat([signal_train_out_e, ttbar_train_out_e], axis=0)\n",
    "val_output = tf.concat([signal_val_out_e, ttbar_val_out_e], axis=0)\n",
    "test_output = tf.concat([signal_test_out_e, ttbar_test_out_e], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nn_model \u001b[39m=\u001b[39m get_model(train_dataset)\n\u001b[1;32m      2\u001b[0m \u001b[39m# #fit the model to train on all but the last column\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMATT, FITTING MODEL\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "nn_model = get_model(train_dataset)\n",
    "# #fit the model to train on all but the last column\n",
    "print(\"MATT, FITTING MODEL\")\n",
    "callback = LearningRateScheduler(custom_LearningRate_schedular)\n",
    "# print(train_dataset[:,train_dataset.shape[1]-1 : train_dataset.shape[1]])\n",
    "nn_fit = nn_model.fit(train_dataset[:, 0:train_dataset.shape[1]-1], train_output, epochs=500, batch_size = 500, validation_data=(val_dataset[:, 0:train_dataset.shape[1]-1], val_output), sample_weight=train_dataset[:,train_dataset.shape[1]-1 : train_dataset.shape[1]], shuffle=True)\n",
    "# validation_data=(val_dataset[:, 0:train_dataset.shape[1]-1], val_output),\n",
    "# print(train_dataset[:,0:train_dataset.shape[1]-1])\n",
    "# nn_fit = nn_model.fit(train_dataset[:,0:train_dataset.shape[1]-1], train_output[:,0:0:train_dataset.shape[1]-1], epochs=70, batch_size=500, verbose=1, shuffle=True, validation_data=(val_dataset[:,0:train_dataset.shape[1]-1], val_output[:,0:train_dataset.shape[1]-1]), sample_weight=train_dataset[:,train_dataset.shape[1]-1:train_dataset.shape[1]])\n",
    "print(\"MATT, MODEL FITTED\")\n",
    "print(\"MATT, PREDICTING\")\n",
    "y_scores = nn_model.predict(test_dataset[:, 0:train_dataset.shape[1]-1])\n",
    "\n",
    "\n",
    "\n",
    "bdt_model = boosted_decision_tree()\n",
    "print(\"MATT, FITTING MODEL\")\n",
    "bdt_fit = bdt_model.fit(train_dataset[:, 0:train_dataset.shape[1]-1], train_output, sample_weight=train_dataset[:,train_dataset.shape[1]-1 : train_dataset.shape[1]])\n",
    "print(\"MATT, MODEL FITTED\")\n",
    "print(\"MATT, PREDICTING\")\n",
    "bdt_y_scores = bdt_model.predict(test_dataset[:, 0:train_dataset.shape[1]-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()\n",
    "# model.make_inspector().features()\n",
    "# print(bdt_model.summary())\n",
    "print(bdt_y_scores)\n",
    "nn_signal_scores = y_scores[test_output == 1]\n",
    "nn_background_scores = y_scores[test_output == 0]\n",
    "\n",
    "bdt_signal_scores = bdt_y_scores[test_output == 1]\n",
    "bdt_background_scores = bdt_y_scores[test_output == 0]\n",
    "\n",
    "nn_fakes, nn_reals, thresholds = roc_curve(test_output, y_scores)\n",
    "bdt_fakes, bdt_reals, bdt_thresholds = roc_curve(test_output, bdt_y_scores)\n",
    "\n",
    "print(\"NN AUC: \", auc(nn_fakes, nn_reals))\n",
    "print(\"BDT_AUC: \", auc(bdt_fakes, bdt_reals))\n",
    "\n",
    "nn_loss_plot = plot_loss(nn_fit)\n",
    "nn_accuracy_plot = plot_accuracy(nn_fit)\n",
    "# bdt_loss_plot = plot_loss(bdt_fit)\n",
    "# bdt_accuracy_plot = plot_accuracy(bdt_fit)\n",
    "# plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)\n",
    "# plotter.plot(nn_fit)\n",
    "\n",
    "plot_roc_curve(nn_fakes, nn_reals)\n",
    "plot_roc_curve(bdt_fakes, bdt_reals)\n",
    "\n",
    "#plot signal and background scores\n",
    "plt.plot(bdt_fakes, bdt_reals, label=\"BDT\")\n",
    "plt.plot(nn_fakes, nn_reals, label=\"NN\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(nn_signal_scores, bins=100, range=(0,1), alpha = 0.5, label='signal')\n",
    "plt.hist(nn_background_scores, bins=100, range=(0,1), alpha = 0.5, label='background')\n",
    "plt.xlabel('NN score')\n",
    "plt.ylabel('a.u.')\n",
    "# plt.yscale('log')\n",
    "plt.show()\n",
    "plt.hist(nn_background_scores, bins=100, range=(0,1), alpha = 0.5, label='background')\n",
    "# plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(bdt_signal_scores, bins=100, range=(0,1), alpha = 0.5, label='signal')\n",
    "plt.xlabel('BDT score')\n",
    "plt.ylabel('a.u.')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "plt.hist(bdt_background_scores, bins=100, range=(0,1), alpha = 0.5, label='background')\n",
    "plt.yscale('log')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
