{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from rbms import DBM\n",
    "import sys\n",
    "sys.path.append(\"../base\")\n",
    "from model import Model\n",
    "from base_func import act_func,out_act_check,Summaries\n",
    "\n",
    "class DBN(Model):\n",
    "    def __init__(self,\n",
    "                 hidden_act_func='relu',\n",
    "                 output_act_func='softmax',\n",
    "                 loss_func='mse',\n",
    "                 struct=[784, 100, 100,10],\n",
    "                 lr=1e-4,\n",
    "                 momentum=0.5,\n",
    "                 use_for='classification',\n",
    "                 bp_algorithm='adam',\n",
    "                 epochs=100,\n",
    "                 batch_size=32,\n",
    "                 dropout=0.3,\n",
    "                 units_type=['gauss','bin'],\n",
    "                 rbm_lr=1e-3,\n",
    "                 rbm_epochs=30,\n",
    "                 cd_k=1,\n",
    "                 pre_train=True):\n",
    "        Model.__init__(self,'DBN')\n",
    "        self.loss_func=loss_func\n",
    "        self.hidden_act_func=hidden_act_func\n",
    "        self.output_act_func = out_act_check(output_act_func,loss_func)\n",
    "        self.use_for=use_for\n",
    "        self.bp_algorithm=bp_algorithm\n",
    "        self.lr=lr\n",
    "        self.momentum=momentum\n",
    "        self.epochs=epochs\n",
    "        self.struct = struct\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.pre_train=pre_train\n",
    "        \n",
    "        self.dbm_struct = struct[:-1]\n",
    "        self.units_type = units_type\n",
    "        self.cd_k = cd_k\n",
    "        self.rbm_lr = rbm_lr\n",
    "        self.rbm_epochs = rbm_epochs\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "    # DBN_model\n",
    "    \n",
    "    def build_model(self):\n",
    "        print(\"Start building model...\")\n",
    "        print('DBN:')\n",
    "        print(self.__dict__)\n",
    "        \"\"\"\n",
    "        Pre-training\n",
    "        \"\"\"\n",
    "        if self.pre_train:\n",
    "            # 构建dbm\n",
    "            self.pt_model = DBM(\n",
    "                    units_type=self.units_type,\n",
    "                    dbm_struct=self.dbm_struct,\n",
    "                    rbm_epochs=self.rbm_epochs,\n",
    "                    batch_size=self.batch_size,\n",
    "                    cd_k=self.cd_k,\n",
    "                    rbm_lr=self.rbm_lr)      \n",
    "        \"\"\"\n",
    "        Fine-tuning\n",
    "        \"\"\"\n",
    "        with tf.name_scope('DBN'):\n",
    "            self.input_data = tf.placeholder(tf.float32, [None, self.struct[0]])\n",
    "            self.label_data = tf.placeholder(tf.float32, [None, self.struct[-1]])\n",
    "            self.keep_prob = tf.placeholder(tf.float32) \n",
    "            self.out_W = tf.Variable(tf.truncated_normal(shape=[self.struct[-2], self.struct[-1]], \n",
    "                                                         stddev=np.sqrt(2 / (self.struct[-2] + self.struct[-1]))), \n",
    "                                                         name='W_out')\n",
    "            self.out_b = tf.Variable(tf.constant(0.0,shape=[self.struct[-1]]),name='b_out')\n",
    "            self.parameter_list = list()\n",
    "            if self.pre_train:\n",
    "                for pt in self.pt_model.pt_list:\n",
    "                    self.parameter_list.append([pt.W,pt.bh])\n",
    "            else:\n",
    "                for i in range(len(self.struct)-2):\n",
    "                    W = tf.Variable(tf.truncated_normal(shape=[self.struct[i], self.struct[i+1]], \n",
    "                                                        stddev=np.sqrt(2 / (self.struct[i] + self.struct[i+1]))), \n",
    "                                                        name='W'+str(i+1))\n",
    "                    b = tf.Variable(tf.constant(0.0,shape=[self.struct[i+1]]),name='b'+str(i+1))\n",
    "                    self.parameter_list.append([W,b])\n",
    "                    \n",
    "            self.parameter_list.append([self.out_W,self.out_b])\n",
    "            \n",
    "            self.logits,self.pred=self.transform(self.input_data)\n",
    "            self.build_train_step()\n",
    "            \n",
    "            if self.tbd:\n",
    "                for i in range(len(self.parameter_list)):\n",
    "                    Summaries.scalars_histogram('_W'+str(i+1),self.parameter_list[i][0])\n",
    "                    Summaries.scalars_histogram('_b'+str(i+1),self.parameter_list[i][1])\n",
    "                tf.summary.scalar('loss',self.loss)\n",
    "                tf.summary.scalar('accuracy',self.accuracy)\n",
    "                self.merge = tf.summary.merge(tf.get_collection(tf.GraphKeys.SUMMARIES,self.name))\n",
    "            \n",
    "    def transform(self,data_x):\n",
    "        next_data = data_x\n",
    "        for i in range(len(self.parameter_list)):\n",
    "            W=self.parameter_list[i][0]\n",
    "            b=self.parameter_list[i][1]\n",
    "            \n",
    "            if self.dropout>0:\n",
    "                next_data = tf.nn.dropout(next_data, self.keep_prob)\n",
    "\n",
    "            z = tf.add(tf.matmul(next_data, W), b)\n",
    "            if i==len(self.parameter_list)-1:\n",
    "                logits=z\n",
    "                output_act=act_func(self.output_act_func)\n",
    "                pred=output_act(z)\n",
    "            else:\n",
    "                hidden_act=act_func(self.hidden_act_func,self.h_act_p)\n",
    "                self.h_act_p = np.mod(self.h_act_p + 1, len(self.hidden_act_func))\n",
    "                next_data=hidden_act(z)\n",
    "            \n",
    "        return logits,pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "from HiddenLayer import HiddenLayer\n",
    "from LogisticRegression import LogisticRegression\n",
    "from RBM import RBM\n",
    "from utils import *\n",
    "\n",
    "\n",
    "class DBN(object):\n",
    "    def __init__(self, input=None, label=None,\\\n",
    "                 n_ins=2, hidden_layer_sizes=[3, 3], n_outs=2,\\\n",
    "                 rng=None):\n",
    "        \n",
    "        self.x = input\n",
    "        self.y = label\n",
    "\n",
    "        self.sigmoid_layers = []\n",
    "        self.rbm_layers = []\n",
    "        self.n_layers = len(hidden_layer_sizes)  # = len(self.rbm_layers)\n",
    "\n",
    "        if rng is None:\n",
    "            rng = numpy.random.RandomState(1234)\n",
    "\n",
    "        \n",
    "        assert self.n_layers > 0\n",
    "\n",
    "\n",
    "        # construct multi-layer\n",
    "        for i in xrange(self.n_layers):\n",
    "            # layer_size\n",
    "            if i == 0:\n",
    "                input_size = n_ins\n",
    "            else:\n",
    "                input_size = hidden_layer_sizes[i - 1]\n",
    "\n",
    "            # layer_input\n",
    "            if i == 0:\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                layer_input = self.sigmoid_layers[-1].sample_h_given_v()\n",
    "                \n",
    "            # construct sigmoid_layer\n",
    "            sigmoid_layer = HiddenLayer(input=layer_input,\n",
    "                                        n_in=input_size,\n",
    "                                        n_out=hidden_layer_sizes[i],\n",
    "                                        rng=rng,\n",
    "                                        activation=sigmoid)\n",
    "            self.sigmoid_layers.append(sigmoid_layer)\n",
    "\n",
    "\n",
    "            # construct rbm_layer\n",
    "            rbm_layer = RBM(input=layer_input,\n",
    "                            n_visible=input_size,\n",
    "                            n_hidden=hidden_layer_sizes[i],\n",
    "                            W=sigmoid_layer.W,     # W, b are shared\n",
    "                            hbias=sigmoid_layer.b)\n",
    "            self.rbm_layers.append(rbm_layer)\n",
    "\n",
    "\n",
    "        # layer for output using Logistic Regression\n",
    "        self.log_layer = LogisticRegression(input=self.sigmoid_layers[-1].sample_h_given_v(),\n",
    "                                            label=self.y,\n",
    "                                            n_in=hidden_layer_sizes[-1],\n",
    "                                            n_out=n_outs)\n",
    "\n",
    "        # finetune cost: the negative log likelihood of the logistic regression layer\n",
    "        self.finetune_cost = self.log_layer.negative_log_likelihood()\n",
    "\n",
    "\n",
    "\n",
    "    def pretrain(self, lr=0.1, k=1, epochs=100):\n",
    "        # pre-train layer-wise\n",
    "        for i in xrange(self.n_layers):\n",
    "            if i == 0:\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                layer_input = self.sigmoid_layers[i-1].sample_h_given_v(layer_input)\n",
    "            rbm = self.rbm_layers[i]\n",
    "            \n",
    "            for epoch in xrange(epochs):\n",
    "                rbm.contrastive_divergence(lr=lr, k=k, input=layer_input)\n",
    "                # cost = rbm.get_reconstruction_cross_entropy()\n",
    "                # print >> sys.stderr, \\\n",
    "                #        'Pre-training layer %d, epoch %d, cost ' %(i, epoch), cost\n",
    "\n",
    "\n",
    "    def finetune(self, lr=0.1, epochs=100):\n",
    "        layer_input = self.sigmoid_layers[-1].sample_h_given_v()\n",
    "\n",
    "        # train log_layer\n",
    "        epoch = 0\n",
    "        done_looping = False\n",
    "        while (epoch < epochs) and (not done_looping):\n",
    "            self.log_layer.train(lr=lr, input=layer_input)\n",
    "            # self.finetune_cost = self.log_layer.negative_log_likelihood()\n",
    "            # print >> sys.stderr, 'Training epoch %d, cost is ' % epoch, self.finetune_cost\n",
    "            \n",
    "            lr *= 0.95\n",
    "            epoch += 1\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        layer_input = x\n",
    "        \n",
    "        for i in xrange(self.n_layers):\n",
    "            sigmoid_layer = self.sigmoid_layers[i]\n",
    "            layer_input = sigmoid_layer.output(input=layer_input)\n",
    "\n",
    "        out = self.log_layer.predict(layer_input)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
