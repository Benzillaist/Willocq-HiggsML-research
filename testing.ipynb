{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 14:02:42.508899: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-25 14:02:43.774888: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/benk/root/root-6.26.06-install/lib\n",
      "2022-10-25 14:02:43.774962: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-25 14:02:43.932624: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-25 14:02:47.319498: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/benk/root/root-6.26.06-install/lib\n",
      "2022-10-25 14:02:47.320018: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/benk/root/root-6.26.06-install/lib\n",
      "2022-10-25 14:02:47.320062: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import uproot\n",
    "import pandas as pd\n",
    "import awkward as ak\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import tensorflow_decision_forests as tfdf\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.models import Sequential\n",
    "from tensorflow import python as tf_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal tree entries: 3319\n",
      "Background tree entries: 3531\n"
     ]
    }
   ],
   "source": [
    "#loads in files for signal and background\n",
    "file_sig = uproot.open(\"mc16e_signal.root\")\n",
    "file_back = uproot.open(\"mc16e_ttbar.root\")\n",
    "\n",
    "#Sets trees of files to variables\n",
    "tree_sig = file_sig[\"nominal\"]\n",
    "tree_back = file_back[\"nominal\"]\n",
    "\n",
    "#Prints number of entries for each tree\n",
    "print(f'Signal tree entries: {tree_sig.num_entries}')\n",
    "print(f'Background tree entries: {tree_back.num_entries}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows contents of each tree\n",
    "#tree_sig.show()\n",
    "#tree_back.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "muonStats_sig = tree_sig.arrays(['mu_pt', 'mu_eta', 'mu_phi'])\n",
    "jetStats_sig = tree_sig.arrays(['jet_pt', 'jet_eta', 'jet_phi'])\n",
    "muonStats_back = tree_back.arrays(['mu_pt', 'mu_eta', 'mu_phi'])\n",
    "jetStats_back = tree_back.arrays(['jet_pt', 'jet_eta', 'jet_phi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109464.52   65954.625 148726.    ...  53041.37   58756.44   56119.04 ]\n"
     ]
    }
   ],
   "source": [
    "print(np.concatenate(np.array(muonStats_sig['mu_pt']), axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(np.concatenate(muonStats_sig['mu_pt'], axis = 0),bins=np.linspace(0,450000,101),label='Signal', histtype='step')\n",
    "#plt.hist(np.concatenate(muonStats_back['mu_pt'], axis = 0),bins=np.linspace(0,450000,101),label='Background', histtype='step')\n",
    "#plt.xlabel(r'Muon $p_{T}$ [GeV]')\n",
    "#plt.ylabel('Count')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mu_pt', 'mu_eta', 'mu_phi', 'ljet_pt', 'ljet_eta', 'ljet_phi', 'ljet_pt_cand', 'ljet_eta_cand', 'ljet_phi_cand']\n",
      "(                        mu_pt    mu_eta    mu_phi   ljet_pt_cand  \\\n",
      "entry subentry                                                     \n",
      "0     0         109464.523438  0.752634 -0.603566  274842.875000   \n",
      "1     0          65954.625000  0.954476  0.651902  212455.109375   \n",
      "2     0         148726.000000  1.510869 -0.182065  371398.625000   \n",
      "3     0          71516.101562  0.254257  2.440928  331628.312500   \n",
      "4     0         222168.671875  0.672102  0.522216  341838.843750   \n",
      "...                       ...       ...       ...            ...   \n",
      "3314  0          52193.957031  0.362391  2.417538  231281.109375   \n",
      "3315  0         146224.984375 -0.150667  0.455322  268928.281250   \n",
      "3316  0          53041.371094 -1.209407  1.332303  219864.437500   \n",
      "3317  0          58756.441406  0.259072  1.774282  221806.578125   \n",
      "3318  0          56119.039062 -0.593785  1.690351  293187.750000   \n",
      "\n",
      "                ljet_eta_cand  ljet_phi_cand  \n",
      "entry subentry                                \n",
      "0     0              0.760044      -0.542501  \n",
      "1     0              1.268582       0.635921  \n",
      "2     0              1.638378      -0.460992  \n",
      "3     0              0.299665       2.942886  \n",
      "4     0              0.488505       0.590382  \n",
      "...                       ...            ...  \n",
      "3314  0              0.386634       1.666222  \n",
      "3315  0             -0.224302       0.574036  \n",
      "3316  0             -1.570454       1.296174  \n",
      "3317  0              0.655477       1.674686  \n",
      "3318  0             -0.087748       1.399554  \n",
      "\n",
      "[3319 rows x 6 columns],                       ljet_pt  ljet_eta  ljet_phi\n",
      "entry subentry                                   \n",
      "0     0         274842.875000  0.760044 -0.542501\n",
      "1     0         212455.109375  1.268582  0.635921\n",
      "2     0         495089.531250  1.834779  2.764217\n",
      "      1         371398.625000  1.638378 -0.460992\n",
      "3     0         407043.968750  0.030081 -0.201683\n",
      "...                       ...       ...       ...\n",
      "3315  0         268928.281250 -0.224302  0.574036\n",
      "3316  0         219864.437500 -1.570454  1.296174\n",
      "3317  0         232007.484375  0.984181 -1.556253\n",
      "      1         221806.578125  0.655477  1.674686\n",
      "3318  0         293187.750000 -0.087748  1.399554\n",
      "\n",
      "[4692 rows x 3 columns])\n"
     ]
    }
   ],
   "source": [
    "print(tree_sig.keys(filter_name=\"/(ljet|mu)_(pt|eta|phi)/\"))\n",
    "allStats_sig = tree_sig.arrays(filter_name=\"/(ljet|mu)_(pt|eta|phi)/\", library = 'pd')\n",
    "allStats_back = tree_back.arrays(filter_name=\"/(ljet|mu)_(pt|eta|phi)/\", library = 'pd')\n",
    "muonStats_sig = allStats_sig[0];\n",
    "jetStats_sig = allStats_sig[1];\n",
    "muonStats_back = allStats_back[0];\n",
    "jetStats_back = allStats_back[1];\n",
    "\n",
    "print(allStats_sig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENSURE WEIGHTS ARE THE LAST ENTRY IN THE VAR ARRAY\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, list_IDs, labelsFunc, batch_size=32, dim=(14), n_channels=1, n_classes=2, shuffle=True):\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labelsFunc = labelsFunc\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X[:, :-1], y, X[:, -1]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.dim,))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        \n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i] = np.load('data/' + str(ID) + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labelsFunc(ID)\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes), \n",
    "\n",
    "#ENSURE WEIGHTS ARE THE LAST ENTRY IN THE VAR ARRAY\n",
    "def getList_ID(fileName, tree, varNames, max_entry = 10000):\n",
    "    df = pd.DataFrame()\n",
    "    opFile = uproot.open(fileName + ':' + tree)\n",
    "    for var in varNames:\n",
    "        varDf = opFile[var].array(entry_stop = max_entry, library = 'pd')\n",
    "        if(varDf.index.nlevels == 2):\n",
    "            df[var] = varDf.reset_index(level=1, drop=True)\n",
    "        else:\n",
    "            df[var] = varDf\n",
    "    saveArr = df.to_numpy()\n",
    "    savePrefix = fileName[:fileName.find('.')]\n",
    "    saveStrings = []\n",
    "    for i in range(saveArr.shape[0]):\n",
    "        saveString = savePrefix + str(i)\n",
    "        saveStrings.append(saveString)\n",
    "        np.save('data/' + saveString + '.npy', saveArr[i])\n",
    "    return saveStrings\n",
    "\n",
    "def create_heat_map(df):\n",
    "    corr = df.corr()\n",
    "    sns.heatmap(corr, \n",
    "    cmap='RdYlGn', \n",
    "    xticklabels=corr.columns.values,\n",
    "    yticklabels=corr.columns.values)\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loss(fit):\n",
    "    plt.plot(fit.history['loss'])\n",
    "    plt.plot(fit.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(fit):\n",
    "    plt.plot(fit.history['accuracy'])\n",
    "    plt.plot(fit.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def custom_LearningRate_schedular(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    else:\n",
    "        return 0.01 * tf.math.exp(0.1 * (10 - epoch))\n",
    "\n",
    "    \n",
    "def get_model(inputShape):\n",
    "    model = keras.Sequential([\n",
    "#    keras.layers.Dense(14, activation='relu', input_shape=inputShape),\n",
    "#    keras.layers.Flatten(),\n",
    "#    keras.layers.Dropout(0.2),\n",
    "#    # keras.layers.Dense(32, activation='relu'),\n",
    "#    keras.layers.Dense(16, activation='relu'),\n",
    "#    # keras.layers.Dense(4, activation='relu'),\n",
    "#    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    tf.keras.layers.InputLayer(input_shape=inputShape),\n",
    "    #tf.keras.layers.Dense(128, activation='relu'),\n",
    "    #tf.keras.layers.Dense(64, activation='relu'),\n",
    "    #tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    #tf.keras.layers.Dense(8, activation='relu'),\n",
    "    #tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=2, activation='softmax')\n",
    "    ])\n",
    "    #model.compile(optimizer=tf.optimizers.SGD(learning_rate=0.0001),\n",
    "    #            loss=tf.keras.losses.BinaryCrossentropy(\n",
    "    #                    name='binary_crossentropy'),\n",
    "    #            metrics=['accuracy', \n",
    "    #                    keras.metrics.AUC(name='auc'),\n",
    "    #                    keras.metrics.AUC(name='prc', curve='PR')])\n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate=1e-2), loss = 'binary_crossentropy', metrics = 'accuracy')\n",
    "    return model\n",
    "\n",
    "def boosted_decision_tree():\n",
    "    model = tfdf.keras.GradientBoostedTreesModel()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_entries = 10000\n",
    "\n",
    "train_variables = ['mupt_cand', 'mueta_cand', 'muphi_cand', 'ljet_pt_cand', 'ljet_eta_cand',\\\n",
    "                   'ljet_phi_cand', 'ljet_mass_cand', 'dR_values_cand', 'pt_higgs',\\\n",
    "                   'mass_T', 'met_met', 'met_phi', 'mass_mj', 'weight']\n",
    "\n",
    "signalMatrix = getList_ID('mc16e_signal.root', 'nominal', train_variables, max_entry = max_entries)\n",
    "signalLabels = np.ones(shape = len(signalMatrix))\n",
    "\n",
    "ttbarMatrix = getList_ID('mc16e_ttbar.root', 'nominal', train_variables, max_entry = max_entries)\n",
    "ttbarLabels = np.zeros(shape = len(ttbarMatrix))\n",
    "\n",
    "mixedMatrix = np.concatenate((signalMatrix, ttbarMatrix))\n",
    "mixedLabels = np.concatenate((signalLabels, ttbarLabels))\n",
    "\n",
    "trainMatrix1, valMatrix, trainLabels1, valLabels = train_test_split(mixedMatrix, mixedLabels, test_size = 0.1)\n",
    "trainMatrix, testMatrix, trainLabels, testLabels = train_test_split(trainMatrix1, trainLabels1, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mc16e_signal2458\n",
      "mc16e_ttbar1815\n"
     ]
    }
   ],
   "source": [
    "print(trainMatrix.__getitem__(1))\n",
    "print(trainMatrix[0])\n",
    "#print(trainLabels)\n",
    "#print(testMatrix)\n",
    "#print(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(ID):\n",
    "    if(\"mc16e_signal\" in ID):\n",
    "        return 1\n",
    "    elif(\"mc16e_ttbar\" in ID):\n",
    "        return 0\n",
    "\n",
    "trainGenerator = DataGenerator(mixedMatrix, getLabel, batch_size=256, dim=14, n_channels=1, n_classes=2, shuffle=True)\n",
    "\n",
    "valGenerator = DataGenerator(valMatrix, getLabel, batch_size=256, dim=14, n_channels=1, n_classes=2, shuffle=True)\n",
    "\n",
    "testGenerator = DataGenerator(testMatrix, getLabel, batch_size=256, dim=14, n_channels=1, n_classes=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 3.62343008e+04,  8.72936100e-02,  1.92481875e-01, ...,\n",
      "         3.75471367e+04, -5.06908774e-01,  1.33178922e+05],\n",
      "       [ 5.98010664e+04, -1.98087931e-01,  1.74881482e+00, ...,\n",
      "         2.99754150e+03,  2.16092229e+00,  1.18696859e+05],\n",
      "       [ 3.46061719e+04,  1.42934370e+00, -1.19033062e+00, ...,\n",
      "         1.25533496e+04, -8.23347047e-02,  1.15611148e+05],\n",
      "       ...,\n",
      "       [ 8.04241328e+04, -1.21859312e-01, -1.46989179e+00, ...,\n",
      "         7.55425547e+04, -5.64514697e-01,  1.46771203e+05],\n",
      "       [ 3.74561367e+04, -7.94552445e-01, -1.39853999e-01, ...,\n",
      "         2.82609922e+04, -5.38902640e-01,  8.45607266e+04],\n",
      "       [ 7.26159922e+04, -8.08387935e-01, -1.96164346e+00, ...,\n",
      "         8.50403984e+04, -2.48496485e+00,  1.09365477e+05]]), array([[1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.]], dtype=float32), array([0.12762275, 0.01162403, 0.01449134, 0.09891679, 0.00372003,\n",
      "       0.00435019, 0.00740623, 0.21985788, 0.00429608, 0.03922915,\n",
      "       0.13029843, 0.11379013, 0.11852824, 0.01182191, 0.00788105,\n",
      "       0.12662829, 0.00521598, 0.12076851, 0.00837399, 0.00658015,\n",
      "       0.13364837, 0.00388346, 0.07724378, 0.1296715 , 0.08921846,\n",
      "       0.01086277, 0.00345138, 0.00772373, 0.12383058, 0.11271889,\n",
      "       0.13029948, 0.091405  , 0.12668536, 0.01086268, 0.11286031,\n",
      "       0.00929309, 0.00127868, 0.10421279, 0.00269824, 0.01577711,\n",
      "       0.12431967, 0.12015271, 0.08527686, 0.00452322, 0.00439334,\n",
      "       0.01594876, 0.0044008 , 0.07256929, 0.12115477, 0.00431709,\n",
      "       0.10136656, 0.00666909, 0.13550736, 0.00860274, 0.10091402,\n",
      "       0.00575287, 0.07025036, 0.14564207, 0.15902136, 0.12650284,\n",
      "       0.00412006, 0.12473972, 0.00857622, 0.00571282, 0.11388085,\n",
      "       0.00793393, 0.1353106 , 0.08253   , 0.00230195, 0.01397548,\n",
      "       0.00371904, 0.10546143, 0.13669735, 0.07948364, 0.04394688,\n",
      "       0.11130529, 0.00683893, 0.00758319, 0.1030983 , 0.01357236,\n",
      "       0.00380199, 0.00345046, 0.12918698, 0.00383832, 0.00695443,\n",
      "       0.00427144, 0.00625686, 0.1366145 , 0.13222119, 0.16098037,\n",
      "       0.11727769, 0.11257522, 0.06807411, 0.00754643, 0.13311666,\n",
      "       0.00458484, 0.11757161, 0.10170316, 0.00612711, 0.00638681,\n",
      "       0.00401084, 0.00642064, 0.00585092, 0.00287055, 0.00929495,\n",
      "       0.12573266, 0.13289157, 0.11480714, 0.19196632, 0.00406396,\n",
      "       0.09780191, 0.00161157, 0.02008745, 0.00570213, 0.12782803,\n",
      "       0.00471044, 0.00037131, 0.00317127, 0.1010793 , 0.00356799,\n",
      "       0.01520445, 0.00525849, 0.00918866, 0.00981299, 0.04564708,\n",
      "       0.13292676, 0.00771905, 0.00451087, 0.13366271, 0.00075086,\n",
      "       0.00417646, 0.11027179, 0.00902437, 0.00345751, 0.00497131,\n",
      "       0.07717443, 0.        , 0.12140859, 0.00601544, 0.00710235,\n",
      "       0.12696248, 0.13350281, 0.00449667, 0.09406632, 0.13612459,\n",
      "       0.13441761, 0.08184994, 0.02302784, 0.00479532, 0.00626308,\n",
      "       0.13688457, 0.00391776, 0.12883201, 0.06103856, 0.02647999,\n",
      "       0.00348821, 0.02577809, 0.00466956, 0.07231349, 0.14631305,\n",
      "       0.00821843, 0.00485009, 0.00634894, 0.13306541, 0.13212162,\n",
      "       0.00717615, 0.00314751, 0.00709793, 0.14259407, 0.0919277 ,\n",
      "       0.13713708, 0.12750049, 0.00348338, 0.01164781, 0.11294851,\n",
      "       0.00383577, 0.00394911, 0.0100991 , 0.00562794, 0.00753786,\n",
      "       0.06909329, 0.01021782, 0.0098314 , 0.14684327, 0.06358337,\n",
      "       0.00451145, 0.14336397, 0.01220698, 0.01186249, 0.0113301 ,\n",
      "       0.1232478 , 0.14022509, 0.09807585, 0.00397599, 0.00610141,\n",
      "       0.0618906 , 0.03031535, 0.07683049, 0.00890493, 0.00640556,\n",
      "       0.10588349, 0.00403022, 0.01048191, 0.01234549, 0.12981401,\n",
      "       0.00756657, 0.13612747, 0.13069904, 0.08001753, 0.12792541,\n",
      "       0.13990488, 0.13739175, 0.01196926, 0.124349  , 0.00499954,\n",
      "       0.1311395 , 0.09130455, 0.00976214, 0.00984086, 0.13353073,\n",
      "       0.0412215 , 0.13472197, 0.        , 0.13380807, 0.00624531,\n",
      "       0.17636365, 0.12013224, 0.00629874, 0.0959649 , 0.12716384,\n",
      "       0.00076761, 0.09217472, 0.07730273, 0.00632542, 0.00538184,\n",
      "       0.00203955, 0.00593886, 0.07075817, 0.0072187 , 0.00488358,\n",
      "       0.00628447, 0.0119514 , 0.12907859, 0.00674499, 0.11549391,\n",
      "       0.00526714, 0.0006807 , 0.00546846, 0.00728736, 0.06075456,\n",
      "       0.0125049 , 0.07217926, 0.01305018, 0.13670116, 0.010378  ,\n",
      "       0.00473642]))\n"
     ]
    }
   ],
   "source": [
    "print(trainGenerator[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 16)                224       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 241\n",
      "Trainable params: 241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn_model = get_model((13,))\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "26/26 [==============================] - 7s 247ms/step - loss: 708.5305 - accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 3s 120ms/step - loss: 187.7516 - accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 3s 125ms/step - loss: 105.0339 - accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 3s 120ms/step - loss: 62.8456 - accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 3s 134ms/step - loss: 55.4452 - accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 5s 179ms/step - loss: 31.3219 - accuracy: 0.5000\n",
      "Epoch 7/100\n",
      " 5/26 [====>.........................] - ETA: 3s - loss: 23.2013 - accuracy: 0.5000"
     ]
    }
   ],
   "source": [
    "nnfit = nn_model.fit(trainGenerator, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 6.14487773e+04,  6.56550109e-01, -7.68729866e-01, ...,\n",
      "         2.33677000e+05, -9.93868887e-01,  9.95781875e+04],\n",
      "       [ 1.24750375e+05, -2.56126970e-02,  1.23471844e+00, ...,\n",
      "         1.17055508e+04,  1.62572753e+00,  9.45575703e+04],\n",
      "       [ 4.25528555e+04,  2.86719501e-01, -2.10796341e-01, ...,\n",
      "         5.65752500e+04, -2.88516045e+00,  8.67482656e+04],\n",
      "       ...,\n",
      "       [ 8.39709688e+04, -5.91560483e-01,  2.42386413e+00, ...,\n",
      "         1.64780469e+05,  2.34537148e+00,  1.06327969e+05],\n",
      "       [ 2.87045449e+04,  9.68313992e-01, -7.95157731e-01, ...,\n",
      "         6.61717109e+04,  1.60240789e-03,  1.10869570e+05],\n",
      "       [ 5.54113555e+04, -2.30690956e-01,  2.49521661e+00, ...,\n",
      "         1.21073877e+04,  5.77773094e-01,  1.11561617e+05]]), array([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
      "       1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "       1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "       0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
      "       1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "       0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "       0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1]), array([0.00406965, 0.10412558, 0.00522442, 0.00970561, 0.11522143,\n",
      "       0.13212162, 0.0690854 , 0.00470448, 0.06961458, 0.08879823,\n",
      "       0.14605048, 0.00321496, 0.11257522, 0.11905124, 0.12307359,\n",
      "       0.10916989, 0.        , 0.00927412, 0.12469029, 0.00820994,\n",
      "       0.00717615, 0.01181426, 0.01054707, 0.13301114, 0.00478321,\n",
      "       0.00840045, 0.00561803, 0.00431709, 0.12211124, 0.00538555,\n",
      "       0.00305063, 0.00223188, 0.00237166, 0.01391728, 0.1249631 ,\n",
      "       0.04979466, 0.00804147, 0.00953904, 0.12057041, 0.00794865,\n",
      "       0.00689408, 0.00748755, 0.11882755, 0.01843605, 0.00266109,\n",
      "       0.03330213, 0.09857996, 0.1394303 , 0.0072979 , 0.13003223,\n",
      "       0.14136311, 0.1319038 , 0.00461947, 0.12792541, 0.00389985,\n",
      "       0.00707156, 0.00465141, 0.00500324, 0.00853324, 0.14447663,\n",
      "       0.00632804, 0.10371767, 0.11278708, 0.14060502, 0.1433641 ,\n",
      "       0.13499907, 0.09418756, 0.00893717, 0.00695264, 0.11958921,\n",
      "       0.00413791, 0.00402636, 0.03218996, 0.00736066, 0.00871431,\n",
      "       0.12886794, 0.13168565, 0.08766756, 0.10251878, 0.00837997,\n",
      "       0.0052277 , 0.01151423, 0.13943524, 0.00399443, 0.00304524,\n",
      "       0.13814294, 0.07523533, 0.11868243, 0.13069904, 0.05563541,\n",
      "       0.02577809, 0.13257346, 0.1234357 , 0.11200232, 0.13165228,\n",
      "       0.0066214 , 0.08298033, 0.1439173 , 0.00664464, 0.13157822,\n",
      "       0.11130326, 0.00737974, 0.07987636, 0.12532597, 0.1332568 ,\n",
      "       0.00617778, 0.12227705, 0.01044715, 0.15467932, 0.034064  ,\n",
      "       0.00401174, 0.01727861, 0.01080426, 0.00767365, 0.00549953,\n",
      "       0.11778039, 0.00763649, 0.00660757, 0.08090045, 0.0199139 ,\n",
      "       0.09491245, 0.06401299, 0.08747914, 0.00301999, 0.05989632,\n",
      "       0.09990331, 0.11297641, 0.00533738, 0.13149962, 0.06203587,\n",
      "       0.00482539, 0.14180669, 0.11023993, 0.00379798, 0.00968015,\n",
      "       0.00518431, 0.01090672, 0.00462358, 0.00407314, 0.00823697,\n",
      "       0.12137399, 0.00648081, 0.11050588, 0.0026724 , 0.06324182,\n",
      "       0.1025846 , 0.11908107, 0.12387231, 0.13216976, 0.13692446,\n",
      "       0.12899494, 0.13045711, 0.0020863 , 0.00879243, 0.14197353,\n",
      "       0.12308049, 0.00611687, 0.10918586, 0.08312855, 0.00922627,\n",
      "       0.1153983 , 0.01406799, 0.08218809, 0.        , 0.12575681,\n",
      "       0.00691174, 0.08535317, 0.00396755, 0.01048191, 0.0095624 ,\n",
      "       0.13681241, 0.07932868, 0.        , 0.00873655, 0.00535679,\n",
      "       0.00554838, 0.00077156, 0.00526674, 0.07703411, 0.00464981,\n",
      "       0.18476145, 0.        , 0.00439334, 0.1367518 , 0.01457921,\n",
      "       0.002821  , 0.07397809, 0.00885971, 0.01644851, 0.14820751,\n",
      "       0.09079431, 0.06943685, 0.00236065, 0.12718155, 0.0038592 ,\n",
      "       0.11272206, 0.10804382, 0.00763338, 0.10778364, 0.00542143,\n",
      "       0.09560412, 0.13447687, 0.11905547, 0.00492378, 0.02761274,\n",
      "       0.01356003, 0.00884316, 0.01700724, 0.00446492, 0.11420226,\n",
      "       0.00303492, 0.07600179, 0.011989  , 0.00326536, 0.11549029,\n",
      "       0.14653628, 0.13989455, 0.13976393, 0.09596913, 0.06706947,\n",
      "       0.11920652, 0.01448007, 0.00979584, 0.07968133, 0.01074114,\n",
      "       0.13485859, 0.13500561, 0.08949717, 0.00735143, 0.06145454,\n",
      "       0.00589866, 0.010378  , 0.00726842, 0.01000315, 0.01632147,\n",
      "       0.09957636, 0.00502962, 0.00551151, 0.13273913, 0.13203583,\n",
      "       0.1219731 , 0.00403022, 0.11459849, 0.1554441 , 0.03502956,\n",
      "       0.00580335, 0.11844568, 0.00375421, 0.01229   , 0.05126626,\n",
      "       0.00441601, 0.11791475, 0.11797122, 0.00341829, 0.13209237,\n",
      "       0.00773795]))\n"
     ]
    }
   ],
   "source": [
    "print(testGenerator[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step\n",
      "[[0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [0.99999994]]\n"
     ]
    }
   ],
   "source": [
    "probability_model = tf.keras.Sequential([nn_model, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "predictions = probability_model.predict(testGenerator[0][0])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn_model = get_model((14,))\n",
    "# #fit the model to train on all but the last column\n",
    "#print(\"MATT, FITTING MODEL\")\n",
    "#callback = LearningRateScheduler(custom_LearningRate_schedular)\n",
    "# print(train_dataset[:,train_dataset.shape[1]-1 : train_dataset.shape[1]])\n",
    "#nn_fit = nn_model.fit(train_dataset[:, 0:train_dataset.shape[1]-1], train_output, epochs=500, batch_size = 500, validation_data=(val_dataset[:, 0:train_dataset.shape[1]-1], val_output), sample_weight=train_dataset[:,train_dataset.shape[1]-1 : train_dataset.shape[1]], shuffle=True)\n",
    "# validation_data=(val_dataset[:, 0:train_dataset.shape[1]-1], val_output),\n",
    "# print(train_dataset[:,0:train_dataset.shape[1]-1])\n",
    "# nn_fit = nn_model.fit(train_dataset[:,0:train_dataset.shape[1]-1], train_output[:,0:0:train_dataset.shape[1]-1], epochs=70, batch_size=500, verbose=1, shuffle=True, validation_data=(val_dataset[:,0:train_dataset.shape[1]-1], val_output[:,0:train_dataset.shape[1]-1]), sample_weight=train_dataset[:,train_dataset.shape[1]-1:train_dataset.shape[1]])\n",
    "#print(\"MATT, MODEL FITTED\")\n",
    "#print(\"MATT, PREDICTING\")\n",
    "#y_scores = nn_model.predict(test_dataset[:, 0:train_dataset.shape[1]-1])\n",
    "\n",
    "\n",
    "\n",
    "#bdt_model = boosted_decision_tree()\n",
    "#print(\"MATT, FITTING MODEL\")\n",
    "#bdt_fit = bdt_model.fit(train_dataset[:, 0:train_dataset.shape[1]-1], train_output, sample_weight=train_dataset[:,train_dataset.shape[1]-1 : train_dataset.shape[1]])\n",
    "#print(\"MATT, MODEL FITTED\")\n",
    "#print(\"MATT, PREDICTING\")\n",
    "#bdt_y_scores = bdt_model.predict(test_dataset[:, 0:train_dataset.shape[1]-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class CIFAR10Sequence(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "\n",
    "        return np.array([\n",
    "            resize(imread(file_name), (200, 200))\n",
    "               for file_name in batch_x]), np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genObj = CIFAR10Sequence([1, 2, 3, 4, 5], [1, 4, 9, 16, 25], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingModel = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(1,)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(62, activation='softmax')\n",
    "])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
