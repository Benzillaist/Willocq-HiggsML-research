{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 14:21:24.712205: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-20 14:21:26.010535: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/benk/root/root-6.26.06-install/lib\n",
      "2022-10-20 14:21:26.010564: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-20 14:21:26.147080: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-20 14:21:29.056818: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/benk/root/root-6.26.06-install/lib\n",
      "2022-10-20 14:21:29.057002: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/benk/root/root-6.26.06-install/lib\n",
      "2022-10-20 14:21:29.057014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import uproot\n",
    "import pandas as pd\n",
    "import awkward as ak\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import tensorflow_decision_forests as tfdf\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.models import Sequential\n",
    "from tensorflow import python as tf_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal tree entries: 3319\n",
      "Background tree entries: 3531\n"
     ]
    }
   ],
   "source": [
    "#loads in files for signal and background\n",
    "file_sig = uproot.open(\"mc16e_signal.root\")\n",
    "file_back = uproot.open(\"mc16e_ttbar.root\")\n",
    "\n",
    "#Sets trees of files to variables\n",
    "tree_sig = file_sig[\"nominal\"]\n",
    "tree_back = file_back[\"nominal\"]\n",
    "\n",
    "#Prints number of entries for each tree\n",
    "print(f'Signal tree entries: {tree_sig.num_entries}')\n",
    "print(f'Background tree entries: {tree_back.num_entries}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows contents of each tree\n",
    "#tree_sig.show()\n",
    "#tree_back.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "muonStats_sig = tree_sig.arrays(['mu_pt', 'mu_eta', 'mu_phi'])\n",
    "jetStats_sig = tree_sig.arrays(['jet_pt', 'jet_eta', 'jet_phi'])\n",
    "muonStats_back = tree_back.arrays(['mu_pt', 'mu_eta', 'mu_phi'])\n",
    "jetStats_back = tree_back.arrays(['jet_pt', 'jet_eta', 'jet_phi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109464.52   65954.625 148726.    ...  53041.37   58756.44   56119.04 ]\n"
     ]
    }
   ],
   "source": [
    "print(np.concatenate(np.array(muonStats_sig['mu_pt']), axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(np.concatenate(muonStats_sig['mu_pt'], axis = 0),bins=np.linspace(0,450000,101),label='Signal', histtype='step')\n",
    "#plt.hist(np.concatenate(muonStats_back['mu_pt'], axis = 0),bins=np.linspace(0,450000,101),label='Background', histtype='step')\n",
    "#plt.xlabel(r'Muon $p_{T}$ [GeV]')\n",
    "#plt.ylabel('Count')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mu_pt', 'mu_eta', 'mu_phi', 'ljet_pt', 'ljet_eta', 'ljet_phi', 'ljet_pt_cand', 'ljet_eta_cand', 'ljet_phi_cand']\n",
      "(                        mu_pt    mu_eta    mu_phi   ljet_pt_cand  \\\n",
      "entry subentry                                                     \n",
      "0     0         109464.523438  0.752634 -0.603566  274842.875000   \n",
      "1     0          65954.625000  0.954476  0.651902  212455.109375   \n",
      "2     0         148726.000000  1.510869 -0.182065  371398.625000   \n",
      "3     0          71516.101562  0.254257  2.440928  331628.312500   \n",
      "4     0         222168.671875  0.672102  0.522216  341838.843750   \n",
      "...                       ...       ...       ...            ...   \n",
      "3314  0          52193.957031  0.362391  2.417538  231281.109375   \n",
      "3315  0         146224.984375 -0.150667  0.455322  268928.281250   \n",
      "3316  0          53041.371094 -1.209407  1.332303  219864.437500   \n",
      "3317  0          58756.441406  0.259072  1.774282  221806.578125   \n",
      "3318  0          56119.039062 -0.593785  1.690351  293187.750000   \n",
      "\n",
      "                ljet_eta_cand  ljet_phi_cand  \n",
      "entry subentry                                \n",
      "0     0              0.760044      -0.542501  \n",
      "1     0              1.268582       0.635921  \n",
      "2     0              1.638378      -0.460992  \n",
      "3     0              0.299665       2.942886  \n",
      "4     0              0.488505       0.590382  \n",
      "...                       ...            ...  \n",
      "3314  0              0.386634       1.666222  \n",
      "3315  0             -0.224302       0.574036  \n",
      "3316  0             -1.570454       1.296174  \n",
      "3317  0              0.655477       1.674686  \n",
      "3318  0             -0.087748       1.399554  \n",
      "\n",
      "[3319 rows x 6 columns],                       ljet_pt  ljet_eta  ljet_phi\n",
      "entry subentry                                   \n",
      "0     0         274842.875000  0.760044 -0.542501\n",
      "1     0         212455.109375  1.268582  0.635921\n",
      "2     0         495089.531250  1.834779  2.764217\n",
      "      1         371398.625000  1.638378 -0.460992\n",
      "3     0         407043.968750  0.030081 -0.201683\n",
      "...                       ...       ...       ...\n",
      "3315  0         268928.281250 -0.224302  0.574036\n",
      "3316  0         219864.437500 -1.570454  1.296174\n",
      "3317  0         232007.484375  0.984181 -1.556253\n",
      "      1         221806.578125  0.655477  1.674686\n",
      "3318  0         293187.750000 -0.087748  1.399554\n",
      "\n",
      "[4692 rows x 3 columns])\n"
     ]
    }
   ],
   "source": [
    "print(tree_sig.keys(filter_name=\"/(ljet|mu)_(pt|eta|phi)/\"))\n",
    "allStats_sig = tree_sig.arrays(filter_name=\"/(ljet|mu)_(pt|eta|phi)/\", library = 'pd')\n",
    "allStats_back = tree_back.arrays(filter_name=\"/(ljet|mu)_(pt|eta|phi)/\", library = 'pd')\n",
    "muonStats_sig = allStats_sig[0];\n",
    "jetStats_sig = allStats_sig[1];\n",
    "muonStats_back = allStats_back[0];\n",
    "jetStats_back = allStats_back[1];\n",
    "\n",
    "print(allStats_sig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENSURE WEIGHTS ARE THE LAST ENTRY IN THE VAR ARRAY\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, list_IDs, labelsFunc, batch_size=32, dim=(14), n_channels=1, n_classes=2, shuffle=True):\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labelsFunc = labelsFunc\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X[:, :-1], y, X[:, -1]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.dim,))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        \n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i] = np.load('data/' + str(ID) + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labelsFunc(ID)\n",
    "\n",
    "        return X, y #keras.utils.to_categorical(y, num_classes=self.n_classes), \n",
    "\n",
    "#ENSURE WEIGHTS ARE THE LAST ENTRY IN THE VAR ARRAY\n",
    "def getList_ID(fileName, tree, varNames, max_entry = 10000):\n",
    "    df = pd.DataFrame()\n",
    "    opFile = uproot.open(fileName + ':' + tree)\n",
    "    for var in varNames:\n",
    "        varDf = opFile[var].array(entry_stop = max_entry, library = 'pd')\n",
    "        if(varDf.index.nlevels == 2):\n",
    "            df[var] = varDf.reset_index(level=1, drop=True)\n",
    "        else:\n",
    "            df[var] = varDf\n",
    "    saveArr = df.to_numpy()\n",
    "    savePrefix = fileName[:fileName.find('.')]\n",
    "    saveStrings = []\n",
    "    for i in range(saveArr.shape[0]):\n",
    "        saveString = savePrefix + str(i)\n",
    "        saveStrings.append(saveString)\n",
    "        np.save('data/' + saveString + '.npy', saveArr[i])\n",
    "    return saveStrings\n",
    "\n",
    "def create_heat_map(df):\n",
    "    corr = df.corr()\n",
    "    sns.heatmap(corr, \n",
    "    cmap='RdYlGn', \n",
    "    xticklabels=corr.columns.values,\n",
    "    yticklabels=corr.columns.values)\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loss(fit):\n",
    "    plt.plot(fit.history['loss'])\n",
    "    plt.plot(fit.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(fit):\n",
    "    plt.plot(fit.history['accuracy'])\n",
    "    plt.plot(fit.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def custom_LearningRate_schedular(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    else:\n",
    "        return 0.01 * tf.math.exp(0.1 * (10 - epoch))\n",
    "\n",
    "    \n",
    "def get_model(inputShape):\n",
    "    model = keras.Sequential([\n",
    "#    keras.layers.Dense(14, activation='relu', input_shape=inputShape),\n",
    "#    keras.layers.Flatten(),\n",
    "#    keras.layers.Dropout(0.2),\n",
    "#    # keras.layers.Dense(32, activation='relu'),\n",
    "#    keras.layers.Dense(16, activation='relu'),\n",
    "#    # keras.layers.Dense(4, activation='relu'),\n",
    "#    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    tf.keras.layers.InputLayer(input_shape=inputShape),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1, activation='softmax')\n",
    "    ])\n",
    "    #model.compile(optimizer=tf.optimizers.SGD(learning_rate=0.0001),\n",
    "    #            loss=tf.keras.losses.BinaryCrossentropy(\n",
    "    #                    name='binary_crossentropy'),\n",
    "    #            metrics=['accuracy', \n",
    "    #                    keras.metrics.AUC(name='auc'),\n",
    "    #                    keras.metrics.AUC(name='prc', curve='PR')])\n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate=1e-6), loss = 'binary_crossentropy', metrics = 'accuracy')\n",
    "    return model\n",
    "\n",
    "def boosted_decision_tree():\n",
    "    model = tfdf.keras.GradientBoostedTreesModel()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_entries = 10000\n",
    "\n",
    "train_variables = ['mupt_cand', 'mueta_cand', 'muphi_cand', 'ljet_pt_cand', 'ljet_eta_cand',\\\n",
    "                   'ljet_phi_cand', 'ljet_mass_cand', 'dR_values_cand', 'pt_higgs',\\\n",
    "                   'mass_T', 'met_met', 'met_phi', 'mass_mj', 'weight']\n",
    "\n",
    "signalMatrix = getList_ID('mc16e_signal.root', 'nominal', train_variables, max_entry = max_entries)\n",
    "signalLabels = np.ones(shape = len(signalMatrix))\n",
    "\n",
    "ttbarMatrix = getList_ID('mc16e_ttbar.root', 'nominal', train_variables, max_entry = max_entries)\n",
    "ttbarLabels = np.zeros(shape = len(ttbarMatrix))\n",
    "\n",
    "mixedMatrix = np.concatenate((signalMatrix, ttbarMatrix))\n",
    "mixedLabels = np.concatenate((signalLabels, ttbarLabels))\n",
    "\n",
    "trainMatrix1, valMatrix, trainLabels1, valLabels = train_test_split(mixedMatrix, mixedLabels, test_size = 0.1)\n",
    "trainMatrix, testMatrix, trainLabels, testLabels = train_test_split(trainMatrix1, trainLabels1, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mc16e_signal1827\n",
      "mc16e_ttbar3469\n"
     ]
    }
   ],
   "source": [
    "print(trainMatrix.__getitem__(1))\n",
    "print(trainMatrix[0])\n",
    "#print(trainLabels)\n",
    "#print(testMatrix)\n",
    "#print(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(ID):\n",
    "    if(\"mc16e_signal\" in ID):\n",
    "        return 1\n",
    "    elif(\"mc16e_ttbar\" in ID):\n",
    "        return 0\n",
    "\n",
    "trainGenerator = DataGenerator(mixedMatrix, getLabel, batch_size=8, dim=14, n_channels=1, n_classes=2, shuffle=False)\n",
    "\n",
    "valGenerator = DataGenerator(valMatrix, getLabel, batch_size=8, dim=14, n_channels=1, n_classes=2, shuffle=False)\n",
    "\n",
    "testGenerator = DataGenerator(testMatrix, getLabel, batch_size=8, dim=14, n_channels=1, n_classes=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 1.09464523e+05,  7.52634048e-01, -6.03565693e-01,\n",
      "         2.74842875e+05,  7.60044038e-01, -5.42500675e-01,\n",
      "         7.87195625e+04,  6.15129620e-02,  3.85847938e+05,\n",
      "         6.01716289e+04,  1.71016719e+04,  9.34258282e-01,\n",
      "         9.34774453e+04],\n",
      "       [ 6.59546250e+04,  9.54476178e-01,  6.51902258e-01,\n",
      "         2.12455109e+05,  1.26858222e+00,  6.35921240e-01,\n",
      "         6.76401641e+04,  3.14512312e-01,  5.75038000e+05,\n",
      "         8.22759570e+03,  2.96790844e+05,  5.93087316e-01,\n",
      "         8.40783906e+04],\n",
      "       [ 1.48726000e+05,  1.51086867e+00, -1.82064682e-01,\n",
      "         3.71398625e+05,  1.63837802e+00, -4.60991949e-01,\n",
      "         7.92862812e+04,  3.06690484e-01,  4.46037438e+05,\n",
      "         2.02403266e+05,  7.20102109e+04,  2.53832960e+00,\n",
      "         1.17009062e+05],\n",
      "       [ 7.15161016e+04,  2.54256874e-01,  2.44092846e+00,\n",
      "         3.31628312e+05,  2.99664795e-01,  2.94288611e+00,\n",
      "         9.84727109e+04,  5.04007280e-01,  4.61401531e+05,\n",
      "         3.06081582e+04,  6.56165547e+04,  2.89154720e+00,\n",
      "         1.32750594e+05],\n",
      "       [ 2.22168672e+05,  6.72102094e-01,  5.22215605e-01,\n",
      "         3.41838844e+05,  4.88505393e-01,  5.90382397e-01,\n",
      "         8.28754062e+04,  1.95842952e-01,  5.16977000e+05,\n",
      "         2.15354844e+05,  5.56850781e+04, -3.12601662e+00,\n",
      "         1.21016172e+05],\n",
      "       [ 3.01775352e+04, -4.70835835e-01, -2.52278543e+00,\n",
      "         3.08400969e+05, -5.09251416e-01, -1.93310320e+00,\n",
      "         9.08657500e+04,  5.90932190e-01,  2.65901062e+05,\n",
      "         8.52216094e+04,  7.61944453e+04,  1.57174397e+00,\n",
      "         1.10430539e+05],\n",
      "       [ 3.00088945e+04, -4.65395480e-01, -1.66670716e+00,\n",
      "         2.56768922e+05,  1.09569751e-01, -9.23079669e-01,\n",
      "         8.27131797e+04,  9.39982355e-01,  3.02629938e+05,\n",
      "         3.03772705e+03,  3.10251719e+04, -1.76630414e+00,\n",
      "         1.19953547e+05],\n",
      "       [ 1.33576422e+05,  5.38780272e-01,  1.23071420e+00,\n",
      "         2.62367750e+05,  8.64369571e-01,  1.23881733e+00,\n",
      "         6.76283516e+04,  3.25690120e-01,  4.34635125e+05,\n",
      "         2.85519785e+04,  4.15413047e+04,  8.45034957e-01,\n",
      "         1.01104391e+05]]), array([1, 1, 1, 1, 1, 1, 1, 1]), array([0.00802538, 0.00594742, 0.00397641, 0.00505997, 0.00082899,\n",
      "       0.00484133, 0.01413668, 0.00135069]))\n"
     ]
    }
   ],
   "source": [
    "print(trainGenerator[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_99 (Dense)            (None, 128)               1792      \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 4)                 68        \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn_model = get_model((13,))\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "856/856 [==============================] - 8s 8ms/step - loss: 12.9334 - accuracy: 0.4847\n",
      "Epoch 2/100\n",
      "856/856 [==============================] - 7s 8ms/step - loss: 6.2381 - accuracy: 0.4847\n",
      "Epoch 3/100\n",
      "144/856 [====>.........................] - ETA: 5s - loss: 2.9863 - accuracy: 0.5278"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [211], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nnfit \u001b[39m=\u001b[39m nn_model\u001b[39m.\u001b[39mfit(trainGenerator, epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1568\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m   1569\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1570\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1572\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \n\u001b[1;32m    465\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 470\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    316\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 340\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    342\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    343\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    387\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 388\u001b[0m     hook(batch, logs)\n\u001b[1;32m    390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1081\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/callbacks.py:1158\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1156\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m-> 1158\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprogbar\u001b[39m.\u001b[39;49mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseen, \u001b[39mlist\u001b[39;49m(logs\u001b[39m.\u001b[39;49mitems()), finalize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/generic_utils.py:1033\u001b[0m, in \u001b[0;36mProgbar.update\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m   1031\u001b[0m info \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m - \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m k\n\u001b[1;32m   1032\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[k], \u001b[39mlist\u001b[39m):\n\u001b[0;32m-> 1033\u001b[0m     avg \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmean(\n\u001b[1;32m   1034\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values[k][\u001b[39m0\u001b[39;49m] \u001b[39m/\u001b[39;49m \u001b[39mmax\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values[k][\u001b[39m1\u001b[39;49m])\n\u001b[1;32m   1035\u001b[0m     )\n\u001b[1;32m   1036\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mabs\u001b[39m(avg) \u001b[39m>\u001b[39m \u001b[39m1e-3\u001b[39m:\n\u001b[1;32m   1037\u001b[0m         info \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m%.4f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m avg\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3429\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3430\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 3432\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_mean(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3433\u001b[0m                       out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:168\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    164\u001b[0m arr \u001b[39m=\u001b[39m asanyarray(a)\n\u001b[1;32m    166\u001b[0m is_float16_result \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m rcount \u001b[39m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[39m=\u001b[39;49mkeepdims, where\u001b[39m=\u001b[39;49mwhere)\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m rcount \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m where \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m umr_any(rcount \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    170\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mMean of empty slice.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mRuntimeWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:77\u001b[0m, in \u001b[0;36m_count_reduce_items\u001b[0;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m axis:\n\u001b[1;32m     76\u001b[0m         items \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mshape[mu\u001b[39m.\u001b[39mnormalize_axis_index(ax, arr\u001b[39m.\u001b[39mndim)]\n\u001b[0;32m---> 77\u001b[0m     items \u001b[39m=\u001b[39m nt\u001b[39m.\u001b[39;49mintp(items)\n\u001b[1;32m     78\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[39m# TODO: Optimize case when `where` is broadcast along a non-reduction\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[39m# axis and full sum is more excessive than needed.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m     \u001b[39m# guarded to protect circular imports\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstride_tricks\u001b[39;00m \u001b[39mimport\u001b[39;00m broadcast_to\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nnfit = nn_model.fit(trainGenerator, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATT, FITTING MODEL\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [183], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m callback \u001b[39m=\u001b[39m LearningRateScheduler(custom_LearningRate_schedular)\n\u001b[1;32m      5\u001b[0m \u001b[39m# print(train_dataset[:,train_dataset.shape[1]-1 : train_dataset.shape[1]])\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m nn_fit \u001b[39m=\u001b[39m nn_model\u001b[39m.\u001b[39mfit(train_dataset[:, \u001b[39m0\u001b[39m:train_dataset\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], train_output, epochs\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, batch_size \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m, validation_data\u001b[39m=\u001b[39m(val_dataset[:, \u001b[39m0\u001b[39m:train_dataset\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], val_output), sample_weight\u001b[39m=\u001b[39mtrain_dataset[:,train_dataset\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m : train_dataset\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]], shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# validation_data=(val_dataset[:, 0:train_dataset.shape[1]-1], val_output),\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# print(train_dataset[:,0:train_dataset.shape[1]-1])\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# nn_fit = nn_model.fit(train_dataset[:,0:train_dataset.shape[1]-1], train_output[:,0:0:train_dataset.shape[1]-1], epochs=70, batch_size=500, verbose=1, shuffle=True, validation_data=(val_dataset[:,0:train_dataset.shape[1]-1], val_output[:,0:train_dataset.shape[1]-1]), sample_weight=train_dataset[:,train_dataset.shape[1]-1:train_dataset.shape[1]])\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMATT, MODEL FITTED\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "nn_model = get_model((14,))\n",
    "# #fit the model to train on all but the last column\n",
    "print(\"MATT, FITTING MODEL\")\n",
    "callback = LearningRateScheduler(custom_LearningRate_schedular)\n",
    "# print(train_dataset[:,train_dataset.shape[1]-1 : train_dataset.shape[1]])\n",
    "nn_fit = nn_model.fit(train_dataset[:, 0:train_dataset.shape[1]-1], train_output, epochs=500, batch_size = 500, validation_data=(val_dataset[:, 0:train_dataset.shape[1]-1], val_output), sample_weight=train_dataset[:,train_dataset.shape[1]-1 : train_dataset.shape[1]], shuffle=True)\n",
    "# validation_data=(val_dataset[:, 0:train_dataset.shape[1]-1], val_output),\n",
    "# print(train_dataset[:,0:train_dataset.shape[1]-1])\n",
    "# nn_fit = nn_model.fit(train_dataset[:,0:train_dataset.shape[1]-1], train_output[:,0:0:train_dataset.shape[1]-1], epochs=70, batch_size=500, verbose=1, shuffle=True, validation_data=(val_dataset[:,0:train_dataset.shape[1]-1], val_output[:,0:train_dataset.shape[1]-1]), sample_weight=train_dataset[:,train_dataset.shape[1]-1:train_dataset.shape[1]])\n",
    "print(\"MATT, MODEL FITTED\")\n",
    "print(\"MATT, PREDICTING\")\n",
    "y_scores = nn_model.predict(test_dataset[:, 0:train_dataset.shape[1]-1])\n",
    "\n",
    "\n",
    "\n",
    "bdt_model = boosted_decision_tree()\n",
    "print(\"MATT, FITTING MODEL\")\n",
    "bdt_fit = bdt_model.fit(train_dataset[:, 0:train_dataset.shape[1]-1], train_output, sample_weight=train_dataset[:,train_dataset.shape[1]-1 : train_dataset.shape[1]])\n",
    "print(\"MATT, MODEL FITTED\")\n",
    "print(\"MATT, PREDICTING\")\n",
    "bdt_y_scores = bdt_model.predict(test_dataset[:, 0:train_dataset.shape[1]-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class CIFAR10Sequence(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "\n",
    "        return np.array([\n",
    "            resize(imread(file_name), (200, 200))\n",
    "               for file_name in batch_x]), np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genObj = CIFAR10Sequence([1, 2, 3, 4, 5], [1, 4, 9, 16, 25], 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
